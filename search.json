[
  {
    "objectID": "index.html#key-differences",
    "href": "index.html#key-differences",
    "title": "",
    "section": "Key Differences",
    "text": "Key Differences\n\nEmbodied AI\n\n\nDefinition: AI with a physical form (robots, self-driving cards) integreated into the environment.\nFocus: Learning and adapting through physical interactions with the environment.\nTech: Real-world sensors and actuators for live perception and action.\nGoal: To adapt and make intelligence decisions through physical engagement.\n\n\n\nPhysical AI\n\n\nDefintion: Any AI that models, reasons, or predicts aspects of the physical world.\nFocus: Understanding and reasoning about physical and virtual worlds.\nTech: Virtual environments, synthetic data generation, and simulation tools.\nGoal: To develop models that understand and predict the dynamics of the physical world.\n\n\n\nü§ñ Speaker notes go here.\nhttps://medium.com/@thevalleylife/ai-terms-explained-embodied-ai-vs-physical-ai-2ad3bec23792\nhttps://www.linkedin.com/pulse/physical-ai-convergence-embodied-living-intelligence-kai-xin-thia-76kkc 1. Embodied AI and the Rise of Generalist Robot Policies 2. The Role of Simulation and Synthetic Data Generation in Physical AI 3. From LLMs to LAMs: Towards Action-Oriented AI 4. Bioengineering and the Potential of Organoid Intelligence\nhttps://www.linkedin.com/pulse/understanding-physical-ai-embodied-agentic-prabhjot-kaur-gosal-ph-d‚Äìojrac * Embodied AI: This is the easiest one to explain. Basically, when AI is integrated into a physical entity, it is called embodied AI (AI in a physical body). Examples include a self-driving car, robot, etc. * Physical AI: I see this as an ecosystem/simulation world that mimics the real world. It essentially provides training field for the embodied AI. I also see this as a synonym of digital twin. * Agentic AI: Agent is any entity that can make decisions autonomously to achieve a certain goal. This goal may or may not require the agent to interact with the physical world/things.\nhttps://www.nvidia.com/en-us/glossary/generative-physical-ai/ * How Does Physical AI Work? * What Is the Role of Reinforcement Learning in Physical AI? * Why Is Physical AI Important? * How Can You Get Started With Physical AI? * Construct a virtual 3D environment * Generate 3D-to-real synthetic data * Train and validate * Deploy"
  },
  {
    "objectID": "index.html#the-role-of-physical-embodied-ai-in-healthcare",
    "href": "index.html#the-role-of-physical-embodied-ai-in-healthcare",
    "title": "",
    "section": "The Role of Physical & Embodied AI in Healthcare",
    "text": "The Role of Physical & Embodied AI in Healthcare\n\nRobotic Surgery\nAI-Enhanced Medical Devices\nHospital and Patient Care Automation\nRehabilitation & Assistive Robots\nRemote Care and Telepresence\nDiagnostics with Physical and Embodied AI\nElderly & Patient Care Robots\n\n\nü§ñ Speaker notes go here."
  },
  {
    "objectID": "index.html#few-benefits-and-challenges",
    "href": "index.html#few-benefits-and-challenges",
    "title": "",
    "section": "Few benefits and challenges",
    "text": "Few benefits and challenges\n\nBenefits\n\n\nGreater precision and efficiency in treatment.\nReduced human error in procedures.\nImproved patient monitoring and personalized care.\nRelief for healthcare workers facing labor shortages.\nAugment human healthcare workers\n\n\n\nChallenges\n\n\nSafety and reliability in high-stakes clinical settings.\nData Privacy and Security\nEthical issues: autonomy, patient trust, and job displacement.\nHigh costs and integration with existing healthcare infrastructure.\nRegulatory hurdles and ethical considerations.\n\n\n\nü§ñ Speaker notes go here."
  },
  {
    "objectID": "index.html#multimodal-sensing",
    "href": "index.html#multimodal-sensing",
    "title": "",
    "section": "Multimodal Sensing",
    "text": "Multimodal Sensing\n\n\nFig 11 from Liu et al.¬†2025 ‚ÄúNeural Brain: A Neuroscience-inspired Framework for Embodied Agents.‚Äù arXiv preprint https://arxiv.org/abs/2505.07634\n\n\nSpeaker notes go here.\n\n\n\nGetting started documentation provide with a range of links to setup, use, run and debug application including github workflow.\n\n\nFigure¬†1"
  },
  {
    "objectID": "index.html#action-mechanisms-of-embodied-ai",
    "href": "index.html#action-mechanisms-of-embodied-ai",
    "title": "",
    "section": "Action mechanisms of Embodied AI",
    "text": "Action mechanisms of Embodied AI\n\n\nFig 15 from Liu et al.¬†2025 ‚ÄúNeural Brain: A Neuroscience-inspired Framework for Embodied Agents.‚Äù arXiv preprint https://arxiv.org/abs/2505.07634\n\n\nSpeaker notes go here."
  },
  {
    "objectID": "index.html#towards-ml-native-data-comsumption",
    "href": "index.html#towards-ml-native-data-comsumption",
    "title": "",
    "section": "Towards ML-native data comsumption",
    "text": "Towards ML-native data comsumption\n\nThe 100,000-year data gap\n\n\nWe would need 100,000 years to match the data that made LLMs possible\n\n\nCollecting the right data,\nCollecting the right way,\nCollecting from day one\n\n\n\nML-native data consumption\n\n\nScalling data-driven robotics is painful\n\n\nRecord everything in asynchronously into MCAP or ROSbags\n\nMCAP/rosbags are optimised for yesterday‚Äôs robotics (ROS1.0) not for the data-driven world of Robotics 2.0 and Physical AI\n\nSyncronise data during recording\n\nImaging discovering your policy learns better at 2x the frequency. Too late, the information is gone\n\n\nML-native data consumption: random access, sharding, batching, frequency-flexible.\n\n\nStephen James on X.com\n\nSep 17, 2025 at 1:00 pm\n\nSep 11, 2025 at 1:00 pm\n\n\n\n\nML-native robotics data platform ‚Äî concrete architecture sketch\nGreat ‚Äî below is a practical, implementation-oriented design you can take to engineers or use as a spec. It starts from MCAP / ROSbag-style recordings and produces ML-native datasets that support random access, sharding, batching, and frequency-flexible reads. I include component diagrams, data formats, indexing, example metadata, reader APIs, sync/resampling strategies, privacy & edge upload, and an MVP ‚Üí scale roadmap.\n1 ‚Äî High-level components (data flow) 2 ‚Äî Storage & file formats (ML-native choices) 3 ‚Äî Episode & chunk indexing model 4 ‚Äî Time synchronization & canonical timeline 5 ‚Äî ML Reader API (Python-style pseudocode) 6 ‚Äî Sharding & distributed training 7 ‚Äî Frequency-flexible consumption 8 ‚Äî Privacy, redaction, and provenance * On-device redaction: face blur, license plate obfuscation, vocal redaction. Save redaction metadata (what was transformed and how). * Provenance: every processed shard stores a chain-of-custody: source_mcap_ids, preprocessing_version, redaction_policy_id, compression_params. * Access control: catalog + object store enforce ACLs. Audit logs for dataset access. 9 ‚Äî Schema & metadata examples 10 ‚Äî Ingest / repair strategies\n11 ‚Äî Performance & cost trade-offs * Chunk size: small chunks (1‚Äì10s) = better random access, more metadata overhead. Large chunks (60‚Äì300s) = fewer objects, better bandwidth for throughput training. Choose based on typical sequence length used by models. * Shard size: tune to match S3/HTTP GET throughput vs number of objects. Typical: 100MB‚Äì1GB per shard. * Local cache: essential for training at scale ‚Äî ephemeral local SSD caching of downloaded shards yields 5‚Äì20x speedup. * Compression: lossy image compression vs storage cost ‚Äî choose per-task.\n12 ‚Äî Implementation roadmap (MVP ‚Üí scale)\nMVP (1‚Äì2 months)\nIngest MCAP ‚Üí extract per-topic timestamps + small index; store raw MCAPs to S3.\nBuild per-episode JSON metadata + small SQLite chunk index.\nImplement a Python reader that can:\nread a chunk,\nmap topic timestamps to canonical time (using simple offsets),\nreturn sequences by canonical-time windows,\ndo nearest-frame selection for images, linear interp for numeric.\nProvide simple shard-by-episode listing and seed-based sampler.\nPhase 2 (3‚Äì6 months)\nImplement chunker + shard packager (Parquet + blob packs).\nAdd time-drift detection and piecewise-linear correction.\nAdd distributed sampler that supports sharding across workers and prefetch cache.\nAdd schema registry and dataset catalog UI.\nPhase 3 (6‚Äì12 months)\nProductionize privacy/redaction on-device with configurable policies.\nAdd hierarchical indexing for long-horizon efficient retrieval.\nAdd advanced resampling (antialias, event syncing), stratified sampling, and integration with orchestration (Kubernetes jobs).\nAdd dataset versioning, snapshot diffs, and integrated labeling tools.\n13 ‚Äî Example quick design decisions & heuristics 14 ‚Äî Quick checklist to get an initial prototype running 15 ‚Äî Shortcomings & research directions * Interpolating images is semantically hard; prefer nearest or simulator-based frame synthesis. * Clock drift in cheap hardware can be non-linear; long sessions may require per-interval recalibration. * Label sparsity remains: aim to co-record human/player feedback and use self-supervised objectives. * Dataset bias & privacy are open problems ‚Äî plan for human-in-the-loop audits."
  },
  {
    "objectID": "index.html#github-getting-started-docs",
    "href": "index.html#github-getting-started-docs",
    "title": "",
    "section": " Github: Getting started docs",
    "text": "Github: Getting started docs\n\n\nFigure¬†2: Getting started documentation provide with a range of links to setup, use, run and debug application including github workflow.\n\nSciortino et al.¬†2017 in Computers in Biology and Medicine https://doi.org/10.1016/j.compbiomed.2017.01.008;\nHe et al.¬†2021 in Front. Med. https://doi.org/10.3389/fmed.2021.729978\n\n\nSpeaker notes go here."
  },
  {
    "objectID": "index.html#title-of-the-slide",
    "href": "index.html#title-of-the-slide",
    "title": "",
    "section": "Title of the slide",
    "text": "Title of the slide\n\nBullet point 1\nBullet point 2\nBullet point 3\n\nBullet point 3.1\nBullet point 3.2\n\n\n\nSciortino et al.¬†2017 in Computers in Biology and Medicine https://doi.org/10.1016/j.compbiomed.2017.01.008;\nHe et al.¬†2021 in Front. Med. https://doi.org/10.3389/fmed.2021.729978\n\n\nNotes goes here"
  },
  {
    "objectID": "index.html#my-journey",
    "href": "index.html#my-journey",
    "title": "",
    "section": "My Journey",
    "text": "My Journey\n\n\n\n\n\n\n\n\nNotes goes here for my journey"
  }
]