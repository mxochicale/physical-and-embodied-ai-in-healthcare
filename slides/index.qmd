---
title: ""
subtitle: ""

format:
  revealjs: 
    slide-number: true
    chalkboard: 
      buttons: true
    controls: false
    width: 1280 #1440 #1920 #1050
    height: 720
    margin: 0.025
    controls-layout: bottom-right
    preview-links: auto
    logo: favicon.svg
    theme: dark
    #css:
      #- css/default.css
      #- css/callouts-html.css
    #footer: <https://quarto.org>
  gfm:
    author: Miguel Xochicale
#revealjs-plugins:
#    - subtitles
---

# {.title-slide .centeredslide background-iframe="https://mxochicale.github.io/web-animations/" loading="lazy"}

::: {style="background-color: rgba(22,22,22,0.75); border-radius: 10px; text-align:center; padding: 0px; padding-left: 1.5em; padding-right: 1.5em; max-width: min-content; min-width: max-content; margin-left: auto; margin-right: auto; padding-top: 0.2em; padding-bottom: 0.2em; line-height: 1.5em!important;"}


<span style="color:#939393; font-size:1.75em; text-align:left; display:block;">
**From Code to Care:**     
The Role of Physical &   
Embodied AI in Healthcare
</span>  

[<br>&nbsp;]{style="padding-bottom: 0.25rem;"}  
[{{< fa solid home >}}](http://mxochicale.github.io/) Miguel Xochicale   
[
  [
  [{{< fa brands github >}} `physical-and-embodied-ai-in-healthcare`](https://mxochicale.github.io/physical-and-embodied-ai-in-healthcare)
  ]{style="border-bottom: 0.5px solid #00ccff;"}
]{style="font-size:0.45em;"}
:::


::: footer
[Q4-2025]{.dim-text style="text-align:left;'}
:::


::: {.notes}
  [[`open-healthcare-slides`](https://github.com/mxochicale/physical-ai-in-healthcare-slides/)]{style="border-bottom: 0.5px solid #00ccff;"}
:::




<!-- *********************** NEW SLIDE *********************** -->
# Overview {style="font-size: 90%;"} 

* [What is Embodied AI and Physical AI?](#epai)
* [Intro to Embodied AI](#intro_eai)
* [Data challenges and opportunities](#data_co)
* [Section title 3](#sectag_title_3)
<!--  Comments -->
<!--  * [Open-Source Software in Healthcare](#sec-ossh) -->





<!-- *********************** NEW SLIDE *********************** -->
# What is Embodied AI and Physical AI? {#epai}
What are their roles in Healthcare?

::: {.notes}
  Notes goes here
:::


<!-- *********************** NEW SLIDE *********************** -->
## Key Differences

::: {.column width="45%"}
Embodied AI

<div style="font-size:0.75em;">

* **Definition:** AI with a physical form (robots, self-driving cards) integreated into the environment.
* **Focus:** Learning and adapting through physical interactions with the environment.
* **Tech:** Real-world sensors and actuators for live perception and action.
* **Goal:** To adapt and make intelligence decisions through physical engagement.
</div>

:::


::: {.column width="45%"}
Physical AI

<div style="font-size:0.75em;">

* **Defintion:** Any AI that models, reasons, or predicts aspects of the physical world.
* **Focus:** Understanding and reasoning about physical and virtual worlds.
* **Tech:** Virtual environments, synthetic data generation, and simulation tools. 
* **Goal:** To develop models that understand and predict the dynamics of the physical world.
</div>

:::


::: {.notes}
ðŸ¤– Speaker notes go here.


https://medium.com/@thevalleylife/ai-terms-explained-embodied-ai-vs-physical-ai-2ad3bec23792


https://www.linkedin.com/pulse/physical-ai-convergence-embodied-living-intelligence-kai-xin-thia-76kkc
1. Embodied AI and the Rise of Generalist Robot Policies
2. The Role of Simulation and Synthetic Data Generation in Physical AI
3. From LLMs to LAMs: Towards Action-Oriented AI
4. Bioengineering and the Potential of Organoid Intelligence

https://www.linkedin.com/pulse/understanding-physical-ai-embodied-agentic-prabhjot-kaur-gosal-ph-d--ojrac
* Embodied AI: This is the easiest one to explain. Basically, when AI is integrated into a physical entity, it is called embodied AI (AI in a physical body). Examples include a self-driving car, robot, etc.
* Physical AI: I see this as an ecosystem/simulation world that mimics the real world. It essentially provides training field for the embodied AI. I also see this as a synonym of digital twin.
* Agentic AI: Agent is any entity that can make decisions autonomously to achieve a certain goal. This goal may or may not require the agent to interact with the physical world/things.


https://www.nvidia.com/en-us/glossary/generative-physical-ai/
* How Does Physical AI Work?
* What Is the Role of Reinforcement Learning in Physical AI?
* Why Is Physical AI Important?
* How Can You Get Started With Physical AI?
  * Construct a virtual 3D environment
  * Generate 3D-to-real synthetic data
  * Train and validate
  * Deploy

:::



<!-- *********************** NEW SLIDE *********************** -->

## The Role of Physical & Embodied AI in Healthcare

* Robotic Surgery
* AI-Enhanced Medical Devices
* Hospital and Patient Care Automation 
* Rehabilitation & Assistive Robots
* Remote Care and Telepresence
* Diagnostics with Physical and Embodied AI
* Elderly & Patient Care Robots

::: {.notes}
ðŸ¤– Speaker notes go here.
:::


<!-- *********************** NEW SLIDE *********************** -->
## Few benefits and challenges

::: {.column width="45%"}
Benefits

<div style="font-size:0.75em;">

* Greater precision and efficiency in treatment.
* Reduced human error in procedures.
* Improved patient monitoring and personalized care.
* Relief for healthcare workers facing labor shortages.
* Augment human healthcare workers

</div>

:::


::: {.column width="45%"}
Challenges

<div style="font-size:0.75em;">

* Safety and reliability in high-stakes clinical settings.
* Data Privacy and Security
* Ethical issues: autonomy, patient trust, and job displacement.
* High costs and integration with existing healthcare infrastructure.
* Regulatory hurdles and ethical considerations.

</div>

:::


::: {.notes}
ðŸ¤– Speaker notes go here.
:::




<!-- *********************** SECTION *********************** -->
# Intro to Embodied AI {#intro_eai}

::: {.notes}
  Notes goes here
:::



<!-- *********************** NEW SLIDE *********************** -->
## Multimodal Sensing


![](figures/multimodal-sensing-liu2025.svg){fig-align=center}



::: {style="font-size: 40%;"}
Fig 11 from **Liu et al. 2025** "Neural Brain: A Neuroscience-inspired Framework for Embodied Agents." arXiv preprint https://arxiv.org/abs/2505.07634
:::


::: {.notes}
Speaker notes go here.

::: {#fig-template}



Getting started documentation provide with a range of links to setup, use, run and debug application including github workflow. 
:::


:::




<!-- *********************** NEW SLIDE *********************** -->
## Action mechanisms of Embodied AI


![](figures/mechanisms-embodiedAI-liu2025.svg){fig-align=center}

::: {style="font-size: 40%;"}
Fig 15 from **Liu et al. 2025** "Neural Brain: A Neuroscience-inspired Framework for Embodied Agents." arXiv preprint https://arxiv.org/abs/2505.07634
:::

::: {.notes}
Speaker notes go here.
:::




<!-- *********************** SECTION *********************** -->
# Data challenges and opportunities {#data_co}
Subsection title

::: {.notes}
  Notes goes here
:::


<!-- *********************** NEW SLIDE *********************** -->
## Towards ML-native data comsumption

::: {.column width="45%"}
The 100,000-year data gap

<div style="font-size:0.75em;">

> We would need 100,000 years to match the data that made LLMs possible

* Collecting the right data,
* Collecting the right way,
* Collecting from day one



</div>

:::


::: {.column width="45%"}
ML-native data consumption

<div style="font-size:0.5em;">

> Scalling data-driven robotics is painful 

* Record everything in asynchronously into MCAP or ROSbags
  * MCAP/rosbags are optimised for yesterday's robotics (ROS1.0)
not for the data-driven world of Robotics 2.0 and Physical AI
* Syncronise data during recording
  * Imaging discovering your policy learns better at 2x the frequency. Too late, the information is gone


**ML-native data consumption: random access, sharding, batching, frequency-flexible.**

</div>

:::


::: {style="font-size: 40%;"}
**Stephen James on [X.com](https://x.com/stepjamUK){target="_blank"}**  

- [Sep 17, 2025 at 1:00 pm](https://x.com/stepjamUK/status/1968283949421179192){target="_blank"}  
- [Sep 11, 2025 at 1:00 pm](https://x.com/stepjamUK/status/1966109482708353421){target="_blank"}  
:::




::: {.notes}

ML-native robotics data platform â€” concrete architecture sketch

Great â€” below is a practical, implementation-oriented design you can take to engineers or use as a spec. It starts from MCAP / ROSbag-style recordings and produces ML-native datasets that support random access, sharding, batching, and frequency-flexible reads. I include component diagrams, data formats, indexing, example metadata, reader APIs, sync/resampling strategies, privacy & edge upload, and an MVP â†’ scale roadmap.

1 â€” High-level components (data flow)
2 â€” Storage & file formats (ML-native choices)
3 â€” Episode & chunk indexing model
4 â€” Time synchronization & canonical timeline
5 â€” ML Reader API (Python-style pseudocode)
6 â€” Sharding & distributed training
7 â€” Frequency-flexible consumption
8 â€” Privacy, redaction, and provenance
	* On-device redaction: face blur, license plate obfuscation, vocal redaction. Save redaction metadata (what was transformed and how).
	* Provenance: every processed shard stores a chain-of-custody: source_mcap_ids, preprocessing_version, redaction_policy_id, compression_params.
	* Access control: catalog + object store enforce ACLs. Audit logs for dataset access.
9 â€” Schema & metadata examples
10 â€” Ingest / repair strategies

11 â€” Performance & cost trade-offs
	* Chunk size: small chunks (1â€“10s) = better random access, more metadata overhead. Large chunks (60â€“300s) = fewer objects, better bandwidth for throughput training. Choose based on typical sequence length used by models.
	* Shard size: tune to match S3/HTTP GET throughput vs number of objects. Typical: 100MBâ€“1GB per shard.
	* Local cache: essential for training at scale â€” ephemeral local SSD caching of downloaded shards yields 5â€“20x speedup.
	* Compression: lossy image compression vs storage cost â€” choose per-task.


12 â€” Implementation roadmap (MVP â†’ scale)

MVP (1â€“2 months)

Ingest MCAP â†’ extract per-topic timestamps + small index; store raw MCAPs to S3.

Build per-episode JSON metadata + small SQLite chunk index.

Implement a Python reader that can:

read a chunk,

map topic timestamps to canonical time (using simple offsets),

return sequences by canonical-time windows,

do nearest-frame selection for images, linear interp for numeric.

Provide simple shard-by-episode listing and seed-based sampler.

Phase 2 (3â€“6 months)

Implement chunker + shard packager (Parquet + blob packs).

Add time-drift detection and piecewise-linear correction.

Add distributed sampler that supports sharding across workers and prefetch cache.

Add schema registry and dataset catalog UI.

Phase 3 (6â€“12 months)

Productionize privacy/redaction on-device with configurable policies.

Add hierarchical indexing for long-horizon efficient retrieval.

Add advanced resampling (antialias, event syncing), stratified sampling, and integration with orchestration (Kubernetes jobs).

Add dataset versioning, snapshot diffs, and integrated labeling tools.

13 â€” Example quick design decisions & heuristics
14 â€” Quick checklist to get an initial prototype running
15 â€” Shortcomings & research directions
	* Interpolating images is semantically hard; prefer nearest or simulator-based frame synthesis.
	* Clock drift in cheap hardware can be non-linear; long sessions may require per-interval recalibration.
	* Label sparsity remains: aim to co-record human/player feedback and use self-supervised objectives.
	* Dataset bias & privacy are open problems â€” plan for human-in-the-loop audits.

:::












<!-- *********************** SECTION *********************** -->
# Section title 3 {#sectag_title_3}
Subsection title

::: {.notes}
  Notes goes here
:::



<!-- *********************** NEW SLIDE *********************** -->
## {{< fa brands github >}} Github: Getting started docs

::: {#fig-template}

![](figures/00_template-vector-images/drawing-v00.svg){fig-align=center}

Getting started documentation provide with a range of links to setup, use, run and debug application including github workflow. 
:::

::: {style="font-size: 40%;"}
**Sciortino et al. 2017** in Computers in Biology and Medicine https://doi.org/10.1016/j.compbiomed.2017.01.008;      
**He et al. 2021** in Front. Med. https://doi.org/10.3389/fmed.2021.729978
:::

::: {.notes}
Speaker notes go here.
:::




<!-- *********************** NEW SLIDE *********************** -->
## Title of the slide

* Bullet point 1
* Bullet point 2
* **Bullet point** 3
  * Bullet point 3.1
  * Bullet point 3.2


::: {style="font-size: 40%;"}
**Sciortino et al. 2017** in Computers in Biology and Medicine https://doi.org/10.1016/j.compbiomed.2017.01.008;      
**He et al. 2021** in Front. Med. https://doi.org/10.3389/fmed.2021.729978
:::
::: {.notes}
  Notes goes here
:::



<!-- *********************** NEW SLIDE *********************** -->
# Extra slides

::: {.notes}
  Notes goes here
:::


<!-- *********************** NEW SLIDE *********************** -->
## My Journey

::: {#sec-mt style="margin-top: 0px; font-size: 50%;"} 
![](figures/mx.svg){fig-align="center" width="100%"}
:::

::: {.notes}
  Notes goes here for my journey
:::
