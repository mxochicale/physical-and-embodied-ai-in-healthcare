Miguel Xochicale

# 

<div style="background-color: rgba(22,22,22,0.75); border-radius: 10px; text-align:center; padding: 0px; padding-left: 1.5em; padding-right: 1.5em; max-width: min-content; min-width: max-content; margin-left: auto; margin-right: auto; padding-top: 0.2em; padding-bottom: 0.2em; line-height: 1.5em!important;">

<span style="color:#939393; font-size:1.75em; text-align:left; display:block;">
**From Code to Care:**  
The Role of Physical &  
Embodied AI in Healthcare </span>

<span style="padding-bottom: 0.25rem;"><br>¬†</span>  
[](http://mxochicale.github.io/) Miguel Xochicale  
<span style="font-size:0.45em;"><span style="border-bottom: 0.5px solid #00ccff;">[
`physical-and-embodied-ai-in-healthcare`](https://mxochicale.github.io/physical-and-embodied-ai-in-healthcare)</span></span>

</div>

<div class="footer">

<span class="dim-text" style="&quot;text-align:left;'">Q4-2025</span>

</div>

<div class="notes">

<span style="border-bottom: 0.5px solid #00ccff;">[`open-healthcare-slides`](https://github.com/mxochicale/physical-ai-in-healthcare-slides/)</span>

</div>

<!-- *********************** NEW SLIDE *********************** -->

# Overview

- [What is Embodied AI and Physical AI?](#epai)
- [Intro to Embodied AI](#intro_eai)
- [Data challenges and opportunities](#data_co)
- [Section title 3](#sectag_title_3) <!--  Comments -->
  <!--  * [Open-Source Software in Healthcare](#sec-ossh) -->

<!-- *********************** NEW SLIDE *********************** -->

# What is Embodied AI and Physical AI?

What are their roles in Healthcare?

<div class="notes">

Notes goes here

</div>

<!-- *********************** NEW SLIDE *********************** -->

## Key Differences

<div class="column" width="45%">

Embodied AI

<div style="font-size:0.75em;">

- **Definition:** AI with a physical form (robots, self-driving cards)
  integreated into the environment.
- **Focus:** Learning and adapting through physical interactions with
  the environment.
- **Tech:** Real-world sensors and actuators for live perception and
  action.
- **Goal:** To adapt and make intelligence decisions through physical
  engagement.

</div>

</div>

<div class="column" width="45%">

Physical AI

<div style="font-size:0.75em;">

- **Defintion:** Any AI that models, reasons, or predicts aspects of the
  physical world.
- **Focus:** Understanding and reasoning about physical and virtual
  worlds.
- **Tech:** Virtual environments, synthetic data generation, and
  simulation tools.
- **Goal:** To develop models that understand and predict the dynamics
  of the physical world.

</div>

</div>

<div class="notes">

ü§ñ Speaker notes go here.

https://medium.com/@thevalleylife/ai-terms-explained-embodied-ai-vs-physical-ai-2ad3bec23792

https://www.linkedin.com/pulse/physical-ai-convergence-embodied-living-intelligence-kai-xin-thia-76kkc 1.
Embodied AI and the Rise of Generalist Robot Policies 2. The Role of
Simulation and Synthetic Data Generation in Physical AI 3. From LLMs to
LAMs: Towards Action-Oriented AI 4. Bioengineering and the Potential of
Organoid Intelligence

https://www.linkedin.com/pulse/understanding-physical-ai-embodied-agentic-prabhjot-kaur-gosal-ph-d‚Äìojrac
\* Embodied AI: This is the easiest one to explain. Basically, when AI
is integrated into a physical entity, it is called embodied AI (AI in a
physical body). Examples include a self-driving car, robot, etc. \*
Physical AI: I see this as an ecosystem/simulation world that mimics the
real world. It essentially provides training field for the embodied AI.
I also see this as a synonym of digital twin. \* Agentic AI: Agent is
any entity that can make decisions autonomously to achieve a certain
goal. This goal may or may not require the agent to interact with the
physical world/things.

https://www.nvidia.com/en-us/glossary/generative-physical-ai/ \* How
Does Physical AI Work? \* What Is the Role of Reinforcement Learning in
Physical AI? \* Why Is Physical AI Important? \* How Can You Get Started
With Physical AI? \* Construct a virtual 3D environment \* Generate
3D-to-real synthetic data \* Train and validate \* Deploy

</div>

<!-- *********************** NEW SLIDE *********************** -->

## The Role of Physical & Embodied AI in Healthcare

- Robotic Surgery
- AI-Enhanced Medical Devices
- Hospital and Patient Care Automation
- Rehabilitation & Assistive Robots
- Remote Care and Telepresence
- Diagnostics with Physical and Embodied AI
- Elderly & Patient Care Robots

<div class="notes">

ü§ñ Speaker notes go here.

</div>

<!-- *********************** NEW SLIDE *********************** -->

## Few benefits and challenges

<div class="column" width="45%">

Benefits

<div style="font-size:0.75em;">

- Greater precision and efficiency in treatment.
- Reduced human error in procedures.
- Improved patient monitoring and personalized care.
- Relief for healthcare workers facing labor shortages.
- Augment human healthcare workers

</div>

</div>

<div class="column" width="45%">

Challenges

<div style="font-size:0.75em;">

- Safety and reliability in high-stakes clinical settings.
- Data Privacy and Security
- Ethical issues: autonomy, patient trust, and job displacement.
- High costs and integration with existing healthcare infrastructure.
- Regulatory hurdles and ethical considerations.

</div>

</div>

<div class="notes">

ü§ñ Speaker notes go here.

</div>

<!-- *********************** SECTION *********************** -->

# Intro to Embodied AI

<div class="notes">

Notes goes here

</div>

<!-- *********************** NEW SLIDE *********************** -->

## Multimodal Sensing

<img src="figures/multimodal-sensing-liu2025.svg"
data-fig-align="center" />

<div style="font-size: 40%;">

Fig 11 from **Liu et al.¬†2025** ‚ÄúNeural Brain: A Neuroscience-inspired
Framework for Embodied Agents.‚Äù arXiv preprint
https://arxiv.org/abs/2505.07634

</div>

<div class="notes">

Speaker notes go here.

<div id="fig-template">

Getting started documentation provide with a range of links to setup,
use, run and debug application including github workflow.

Figure¬†1

</div>

</div>

<!-- *********************** NEW SLIDE *********************** -->

## Action mechanisms of Embodied AI

<img src="figures/mechanisms-embodiedAI-liu2025.svg"
data-fig-align="center" />

<div style="font-size: 40%;">

Fig 15 from **Liu et al.¬†2025** ‚ÄúNeural Brain: A Neuroscience-inspired
Framework for Embodied Agents.‚Äù arXiv preprint
https://arxiv.org/abs/2505.07634

</div>

<div class="notes">

Speaker notes go here.

</div>

<!-- *********************** SECTION *********************** -->

# Data challenges and opportunities

Subsection title

<div class="notes">

Notes goes here

</div>

<!-- *********************** NEW SLIDE *********************** -->

## Towards ML-native data comsumption

<div class="column" width="45%">

The 100,000-year data gap

<div style="font-size:0.75em;">

> We would need 100,000 years to match the data that made LLMs possible

- Collecting the right data,
- Collecting the right way,
- Collecting from day one

</div>

</div>

<div class="column" width="45%">

ML-native data consumption

<div style="font-size:0.5em;">

> Scalling data-driven robotics is painful

- Record everything in asynchronously into MCAP or ROSbags
  - MCAP/rosbags are optimised for yesterday‚Äôs robotics (ROS1.0) not for
    the data-driven world of Robotics 2.0 and Physical AI
- Syncronise data during recording
  - Imaging discovering your policy learns better at 2x the frequency.
    Too late, the information is gone

**ML-native data consumption: random access, sharding, batching,
frequency-flexible.**

</div>

</div>

<div style="font-size: 40%;">

**Stephen James on
<a href="https://x.com/stepjamUK" target="_blank">X.com</a>**

- <a href="https://x.com/stepjamUK/status/1968283949421179192"
  target="_blank">Sep 17, 2025 at 1:00 pm</a>  
- <a href="https://x.com/stepjamUK/status/1966109482708353421"
  target="_blank">Sep 11, 2025 at 1:00 pm</a>  

</div>

<div class="notes">

ML-native robotics data platform ‚Äî concrete architecture sketch

Great ‚Äî below is a practical, implementation-oriented design you can
take to engineers or use as a spec. It starts from MCAP / ROSbag-style
recordings and produces ML-native datasets that support random access,
sharding, batching, and frequency-flexible reads. I include component
diagrams, data formats, indexing, example metadata, reader APIs,
sync/resampling strategies, privacy & edge upload, and an MVP ‚Üí scale
roadmap.

1 ‚Äî High-level components (data flow) 2 ‚Äî Storage & file formats
(ML-native choices) 3 ‚Äî Episode & chunk indexing model 4 ‚Äî Time
synchronization & canonical timeline 5 ‚Äî ML Reader API (Python-style
pseudocode) 6 ‚Äî Sharding & distributed training 7 ‚Äî Frequency-flexible
consumption 8 ‚Äî Privacy, redaction, and provenance \* On-device
redaction: face blur, license plate obfuscation, vocal redaction. Save
redaction metadata (what was transformed and how). \* Provenance: every
processed shard stores a chain-of-custody: source_mcap_ids,
preprocessing_version, redaction_policy_id, compression_params. \*
Access control: catalog + object store enforce ACLs. Audit logs for
dataset access. 9 ‚Äî Schema & metadata examples 10 ‚Äî Ingest / repair
strategies

11 ‚Äî Performance & cost trade-offs \* Chunk size: small chunks (1‚Äì10s) =
better random access, more metadata overhead. Large chunks (60‚Äì300s) =
fewer objects, better bandwidth for throughput training. Choose based on
typical sequence length used by models. \* Shard size: tune to match
S3/HTTP GET throughput vs number of objects. Typical: 100MB‚Äì1GB per
shard. \* Local cache: essential for training at scale ‚Äî ephemeral local
SSD caching of downloaded shards yields 5‚Äì20x speedup. \* Compression:
lossy image compression vs storage cost ‚Äî choose per-task.

12 ‚Äî Implementation roadmap (MVP ‚Üí scale)

MVP (1‚Äì2 months)

Ingest MCAP ‚Üí extract per-topic timestamps + small index; store raw
MCAPs to S3.

Build per-episode JSON metadata + small SQLite chunk index.

Implement a Python reader that can:

read a chunk,

map topic timestamps to canonical time (using simple offsets),

return sequences by canonical-time windows,

do nearest-frame selection for images, linear interp for numeric.

Provide simple shard-by-episode listing and seed-based sampler.

Phase 2 (3‚Äì6 months)

Implement chunker + shard packager (Parquet + blob packs).

Add time-drift detection and piecewise-linear correction.

Add distributed sampler that supports sharding across workers and
prefetch cache.

Add schema registry and dataset catalog UI.

Phase 3 (6‚Äì12 months)

Productionize privacy/redaction on-device with configurable policies.

Add hierarchical indexing for long-horizon efficient retrieval.

Add advanced resampling (antialias, event syncing), stratified sampling,
and integration with orchestration (Kubernetes jobs).

Add dataset versioning, snapshot diffs, and integrated labeling tools.

13 ‚Äî Example quick design decisions & heuristics 14 ‚Äî Quick checklist to
get an initial prototype running 15 ‚Äî Shortcomings & research directions
\* Interpolating images is semantically hard; prefer nearest or
simulator-based frame synthesis. \* Clock drift in cheap hardware can be
non-linear; long sessions may require per-interval recalibration. \*
Label sparsity remains: aim to co-record human/player feedback and use
self-supervised objectives. \* Dataset bias & privacy are open problems
‚Äî plan for human-in-the-loop audits.

</div>

<!-- *********************** SECTION *********************** -->

# Section title 3

Subsection title

<div class="notes">

Notes goes here

</div>

<!-- *********************** NEW SLIDE *********************** -->

##  Github: Getting started docs

<div id="fig-template">

<img src="figures/00_template-vector-images/drawing-v00.svg"
data-fig-align="center" />

Figure¬†2: Getting started documentation provide with a range of links to
setup, use, run and debug application including github workflow.

</div>

<div style="font-size: 40%;">

**Sciortino et al.¬†2017** in Computers in Biology and Medicine
https://doi.org/10.1016/j.compbiomed.2017.01.008;  
**He et al.¬†2021** in Front. Med.
https://doi.org/10.3389/fmed.2021.729978

</div>

<div class="notes">

Speaker notes go here.

</div>

<!-- *********************** NEW SLIDE *********************** -->

## Title of the slide

- Bullet point 1
- Bullet point 2
- **Bullet point** 3
  - Bullet point 3.1
  - Bullet point 3.2

<div style="font-size: 40%;">

**Sciortino et al.¬†2017** in Computers in Biology and Medicine
https://doi.org/10.1016/j.compbiomed.2017.01.008;  
**He et al.¬†2021** in Front. Med.
https://doi.org/10.3389/fmed.2021.729978

</div>

<div class="notes">

Notes goes here

</div>

<!-- *********************** NEW SLIDE *********************** -->

# Extra slides

<div class="notes">

Notes goes here

</div>

<!-- *********************** NEW SLIDE *********************** -->

## My Journey

<div id="sec-mt" style="margin-top: 0px; font-size: 50%;">

<img src="figures/mx.svg" style="width:100.0%"
data-fig-align="center" />

</div>

<div class="notes">

Notes goes here for my journey

</div>
